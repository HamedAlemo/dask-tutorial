{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f350668-556d-495f-8f90-3bce64dc6595",
   "metadata": {},
   "source": [
    "# Introduction to Dask DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fa4249-ea9b-434d-8a58-c20f9e104f51",
   "metadata": {},
   "source": [
    "Dask DataFrames coordinate many pandas DataFrames/Series arranged along the index. A Dask DataFrame is partitionedÂ row-wise, grouping rows by index value for efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0aadce-d8b6-40c2-ab77-5fdd2ee488c4",
   "metadata": {},
   "source": [
    "![Dask DataFrame](https://docs.dask.org/en/stable/_images/dask-dataframe.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e112b4-0172-4970-a9d8-14eb5dbd15ef",
   "metadata": {},
   "source": [
    "Many of existing methods from pandas API are available in Dask DataFrame. Checkout [this section](https://docs.dask.org/en/stable/dataframe.html#scope) of the documentation to learn more about these. In general, computations that are parallelizable are implemented in Dask DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4362f85f-eb93-44ac-a9fc-b1b5648fd57e",
   "metadata": {},
   "source": [
    "In this lecture, you will learn to use Dask DataFrame to analyze large tabular climate data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867ad78f-66b5-4ee7-91e0-15ee71de1fc1",
   "metadata": {},
   "source": [
    "## Analyzing Multiple Large CSV files using Dask Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52fa4bd-3ab4-4114-96f4-4ed5d273a3cb",
   "metadata": {},
   "source": [
    "For this tutorial, we will use the NOAA Global Historical Climatology Network Daily (GHCN-D) data available on AWS S3. \n",
    "You can reach more about the data on Registry of Open Data on AWS [here](https://registry.opendata.aws/noaa-ghcn/).\n",
    "\n",
    "More information about the dataset, including the metadata descriptions, is available on [NOAA's website](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily). \n",
    "\n",
    "GHCN-D contains **daily observations** over global land areas. It contains station-based measurements from land-based stations worldwide, about two thirds of which are for precipitation measurement only. Some data are more than *175 years* old."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc7caba-5228-41af-92d2-c98eeea7e0db",
   "metadata": {
    "tags": []
   },
   "source": [
    "This dataset is very large and to analyze it within Python you need to use Dask Dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6650833-b6bf-471a-8954-c488c27b8211",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download Data from AWS S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b677e4-9b22-406b-97d1-1424625bb275",
   "metadata": {},
   "source": [
    "You can download the dataset from AWS S3 bucket using the following commands. This dataset does not require an AWS account (hence the `--no-sign-request` flag should be passed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3bc9c7-0072-40f5-b0ab-a7b9f48b4ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "\n",
    "def download_s3_objects_no_auth(bucket_name, download_path, prefix, substring, aws_region=None):\n",
    "    \"\"\"\n",
    "    Download all objects from a public S3 bucket (no authentication) that contain a specific prefix and substring in their keys.\n",
    "\n",
    "    :param bucket_name: The name of the S3 bucket.\n",
    "    :param download_path: Local directory where the files will be downloaded.\n",
    "    :param prefix: Characters that are required to be at the begining of the S3 object keys.\n",
    "    :param substring: The substring to search for in the S3 object keys.\n",
    "    :param aws_region: AWS region where the S3 bucket is located (optional).\n",
    "    \"\"\"\n",
    "    # Initialize the S3 client with no request signing (public bucket)\n",
    "    s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED), region_name=aws_region)\n",
    "\n",
    "    # Ensure the download path exists\n",
    "    if not os.path.exists(download_path):\n",
    "        os.makedirs(download_path)\n",
    "\n",
    "    # List all objects in the bucket that contain the prefix\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    if 'Contents' not in response:\n",
    "        print(f\"No objects found in the bucket '{bucket_name}'.\")\n",
    "        return\n",
    "\n",
    "    # Loop through objects and download those that contain the substring\n",
    "    for obj in response['Contents']:\n",
    "        key = obj['Key']\n",
    "        if substring in key:\n",
    "            local_filename = os.path.join(download_path, key.split('/')[-1])\n",
    "            print(f\"Downloading {key} to {local_filename}...\")\n",
    "            s3_client.download_file(bucket_name, key, local_filename)\n",
    "            print(f\"Downloaded: {local_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e07720-fb46-43f9-9307-bef460c110f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_s3_objects_no_auth(bucket_name=\"noaa-ghcn-pds\", download_path=\".\", prefix=\"csv/by_year/\", substring=\"202\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a74dadf-5d3d-4224-895c-c34ad46ccd5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b643b-48c1-4b48-992e-b865b5962ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75332b54-8b3e-4e00-b405-9f8c75144ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca621f69-fe73-45b6-9ae5-ff53a8186123",
   "metadata": {},
   "source": [
    "### Read One CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b2ca93-70c2-47f4-b75e-fbce0e7bc6c8",
   "metadata": {},
   "source": [
    "Let's first load one CSV file and see how Dask Dataframe works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452d022d-e62d-4329-9640-fea39170b93f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = dd.read_csv(\"2023.csv\", dtype={'Q_FLAG': 'object'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3829d0e-567f-4f8c-b5df-85a6766ae35c",
   "metadata": {},
   "source": [
    "You can check the number of partitions that Dask by defualt selects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd497f80-9942-42ec-b36a-ad25742cccc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.npartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8752de7e-6e81-4298-836f-6d47c260ea32",
   "metadata": {},
   "source": [
    "To change the number of partitions you need to define the `blocksize` in the `read_csv` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb10290-4be6-49c5-b580-61d3ae0f2838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = dd.read_csv(\"2023.csv\", dtype={'Q_FLAG': 'object'}, blocksize=25e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e64bcca-1475-4443-bcc4-725f8dac24c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.npartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6af28c-78df-49a8-9afb-b2bc6b1d6600",
   "metadata": {},
   "source": [
    "The following line is an unnecessary step and you should not do it regularly. \n",
    "We will just try it to see how `dd` loads the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094fe96-55cb-4151-825a-b754a0e4ac14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3967cbd-4965-49b8-9c0e-7983f797fb66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5752139a-fe3f-46f0-a643-b723aa5fc54c",
   "metadata": {},
   "source": [
    "As you see, `df` is empty again. This is because Dask does not store the outputs of the `df.compute()` back in `df`. If you need to keep these values, you should instead run `df = df.compute()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8bda25-43f0-49f2-8caa-6f4e0d55f6a4",
   "metadata": {},
   "source": [
    "### Read Multiple CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef506c2a-b101-4a1a-8eb5-e5c3be974fed",
   "metadata": {},
   "source": [
    "Here, we will define a new df and load multiple CSV files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12bb7db-66ec-4185-b976-433b0bbd6bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "large_df = dd.read_csv(\"*.csv\", dtype={'Q_FLAG': 'object'}, blocksize=25e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed7865-916d-477d-acf1-f206279acb48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "large_df.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a829bca-5ccc-4f1d-8762-d2c63346ca23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "large_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a22191-f9be-46b9-be65-396c1fa39b43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is going to fail, do not run it. \n",
    "# large_df.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcedaea-419d-438d-ae0a-c9f120d066a3",
   "metadata": {},
   "source": [
    "Let's calculate the mean of each type of observation in whole dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d2761-3810-4020-b01d-d051918916b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_values = large_df.groupby(\"ELEMENT\")[\"DATA_VALUE\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507c91f-814b-4c80-a875-327a9da8dedb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242dfa6f-6be8-4da0-8f5b-6c84ee62cf51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_values.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac0694e-0280-4a34-9313-6c148c4816e0",
   "metadata": {},
   "source": [
    "Next, we will select a station in Worcester, MA and calculate the mean for each observation. \n",
    "You can see the list of all stations on NOAA's website [here](https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad68ba6-9a52-4925-86fa-21fda28a0525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "worcester_df = large_df[large_df[\"ID\"].isin([\"US1MAWR0097\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eaec1c-b2b6-4c11-8edf-195b7a8e7f53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "worcester_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f100d120-6fc1-445f-9b30-669081f65200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "worcester_mean = worcester_df.groupby(\"ELEMENT\")[\"DATA_VALUE\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ec22a0-f675-4163-91e3-9f38fae52d43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "worcester_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb21958-9bf2-4b8f-9717-18dde7927c8e",
   "metadata": {},
   "source": [
    "Now, we want to calculate the mean but we are interested to keep these values in memory. So we will assign the output to a new variable `worcester_mean_values`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e38aadf-384c-4b1f-8c04-3982a32986e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "worcester_mean_values = worcester_mean.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd28f16-720f-4807-8b37-a0c2197cfab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "worcester_mean_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcf63e0-df71-4043-8054-0a45cb89f84f",
   "metadata": {},
   "source": [
    "### Task: find the station with the highest number of snow days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37be823c-a71d-4c5a-a421-7c9fe0d6b505",
   "metadata": {},
   "source": [
    "In the following, we aim to find the station that has the highest number of snow days across years 2020-2024:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a8798-e119-4b25-bc6d-bee40f707b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
